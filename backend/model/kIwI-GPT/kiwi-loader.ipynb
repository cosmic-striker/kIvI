{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "block_size = 64\n",
    "batches = 32\n",
    "train_loop = 5000\n",
    "learning_rate = 5e-4\n",
    "train_split = 0.8\n",
    "debug_loop = 500\n",
    "embeds = 512\n",
    "heads = 4\n",
    "layers = 8\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '}', '´', 'Á', 'à', 'á', 'æ', 'è', 'é', 'í', 'ò', 'ó', 'ù', 'ú', 'Ā', 'ā', 'Ă', 'ă', 'ē', 'ĕ', 'ħ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'ō', 'Ŏ', 'ŏ', 'œ', 'Ū', 'ū', 'ŭ', 'ȳ', '̄', '̆', 'Α', 'Κ', 'Λ', 'Ν', 'Ο', 'Τ', '‘', '’', '“', '”', '⪥', '\\ufeff']\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open('grammar.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([125,   0,   0,   0,   0,  39,  71,  68,  57,  74,  56,  58,  57,   1,\n",
      "         55,  78,   1,  26,  61,  71,  62,  72,   1,  26,  74,  71,  67,  68,\n",
      "         76,   1,  54,  67,  57,   1,  73,  61,  58,   1,  38,  67,  65,  62,\n",
      "         67,  58,   1,  27,  62,  72,  73,  71,  62,  55,  74,  73,  58,  57,\n",
      "          0,  39,  71,  68,  68,  59,  71,  58,  54,  57,  62,  67,  60,   1,\n",
      "         43,  58,  54,  66,   1,  54,  73,   1,  61,  73,  73,  69,  21,  10,\n",
      "         10,  76,  76,  76,   9,  69,  60,  57,  69,   9,  67,  58,  73,   1,\n",
      "          4,  43], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "str2int = { ch:i for i,ch in enumerate(chars) }\n",
    "int2str = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [ str2int[c] for c in s ]\n",
    "decode = lambda l: ''.join([ int2str[i] for i in l ])\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[ 73,  61,  58,  ...,  71,  75,  58],\n",
      "        [ 66,  74,  56,  ...,  58,   1, 122],\n",
      "        [  0,  62,  67,  ...,  68,  74,  73],\n",
      "        ...,\n",
      "        [ 73,  68,   1,  ...,   1,  67,  62],\n",
      "        [ 58,   7,   1,  ...,  68,  76,  58],\n",
      "        [ 68,  59,   1,  ...,  25,  71,  68]], device='cuda:0')\n",
      "target:\n",
      "tensor([[ 61,  58,   0,  ...,  75,  58,   1],\n",
      "        [ 74,  56,  61,  ...,   1, 122,  69],\n",
      "        [ 62,  67,  59,  ...,  74,  73,  52],\n",
      "        ...,\n",
      "        [ 68,   1,  73,  ...,  67,  62,  56],\n",
      "        [  7,   1,  69,  ...,  76,  58,  75],\n",
      "        [ 59,   1,  73,  ...,  71,  68,  76]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(train_split*len(data))\n",
    "train = data[:n]\n",
    "valid = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split=='train' else valid\n",
    "    ix = torch.randint(len(data) - block_size, (batches,))\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "print('target:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrbro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embeds, head_size, bias=False)\n",
    "        self.query = nn.Linear(embeds, head_size, bias=False)\n",
    "        self.value = nn.Linear(embeds, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, embeds)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embeds, 4*embeds),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embeds, embeds),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embeds, heads):\n",
    "        super().__init__()\n",
    "        head_size = embeds // heads\n",
    "        self.sa = MultiHeadAttention(heads, head_size)\n",
    "        self.ffwd = FeedForward(embeds)\n",
    "        self.ln1 = nn.LayerNorm(embeds)\n",
    "        self.ln2 = nn.LayerNorm(embeds)\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "class Kiwi(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embeds)\n",
    "        self.position_embedding = nn.Embedding(block_size, embeds)\n",
    "        self.blocks = nn.Sequential(*[Block(embeds, heads=heads) for _ in range(layers)])\n",
    "        self.final_norm = nn.LayerNorm(embeds)\n",
    "        self.head = nn.Linear(embeds, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "        tok = self.token_embedding(index)\n",
    "        pos = self.position_embedding(torch.arange(T, device=device))\n",
    "        x = tok + pos\n",
    "        x = self.blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for iter in range(max_new_tokens):\n",
    "            index_crop = index[:, -block_size:]\n",
    "            logits, loss = self.forward(index_crop)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "            print(f\"\\r<{'*' * int(10 * iter/max_new_tokens)}{' ' * (10 - int(10*iter/max_new_tokens))}>\", end='', flush=False)\n",
    "        print(\"\\r<**********>\")\n",
    "        return index\n",
    "\n",
    "with open('kiwi.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<**********>\n",
      "\n",
      "\n",
      "\n",
      "  Your of Knoxed’s jokes CockI of Rading’s Etology!”\n",
      "\n",
      "So nothing like and “niasguable (that existady,”--“I pudding vitels, and so that.”\n",
      "\n",
      "The Perfective passates or Luveramu’s I skeen as very lok ardo.\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "SECTION III.\n",
      "OF VERBS.\n",
      "\n",
      "Nominative, governed little of those: asPhasis, and This, Disjunctive, Jilia,\n",
      "Draēlitya&c.\n",
      "\n",
      "\n",
      "\n",
      "COnjunction knews with a preson in takin which the gentmetian of skulenby. For it is not uts\n",
      "than a nice other worth is and that every of the formed, which the\n",
      "Comicalities muse, quite draw, and dravkabuting addressed afford a\n",
      "personsion, or requirent know. Avering it to a celtbetive to _doe_ the heat--ewhich will the slowly into-morrious principle will you and sell\n",
      "reasticate; and whom drunked and as last nent case in the affact,\n",
      "  Wermarkins, but are some vulgar is mistles body excited of\n",
      "   ERáVE MIST AND OF wIRD VESS INTENSS,\n",
      "\n",
      "  FRONINubs TO BENGo,\n",
      "   Lord R. Pentons of which To relatice, Victud, indev0 Scound,\n",
      "    would say that dance? foodmed on \n",
      "CPU times: total: 37 s\n",
      "Wall time: 51.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ctx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated = decode(model.generate(ctx, max_new_tokens=1000)[0].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<**********>\n",
      "He is a dessertative of them, which, in conduce, and green, number, “William a\n",
      "spard,” &c.\n",
      "\n",
      "SECTION SUBJECE.\n",
      "\n",
      "1. Wo pronouns relate for prouns which we shall toolowitys as\n",
      "shocking of certain and that alread supor pronounciations: at is a\n",
      "“Dictor jock that gives;”\n",
      "the Brothong make, that which can scorreover dispose. _Bin_, _any_, _an_, _an_, _a_, _an_, _en_:_, are,\n",
      "and _are_ inspectably the addjective, and of the substantives; and althengs it is\n",
      "also hing which in the definitive comparative shall never\n"
     ]
    }
   ],
   "source": [
    "prompt = 'He is a '\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
